{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "After having implemented our solution, we want to know how it does compared to the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aristizabal95/programming/MediQA/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:05<00:00,  1.56it/s]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "from mediqa.config.core import config\n",
    "from mediqa.rag.manager import VectorDBManager\n",
    "from mediqa.rag.reader import Reader\n",
    "\n",
    "vdb_manager = VectorDBManager(config.rag_config)\n",
    "reader = Reader(config.reader_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation set\n",
    "\n",
    "We will first compute scores on the validation set that was used to compare the multiple models of choice. This will allow us to see how the additions affect performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "val_mini_df = pd.read_csv(\"../data/val_mini.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [03:22<00:00, 10.14s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "responses = []\n",
    "for i, row in tqdm(val_mini_df.iterrows(), total=len(val_mini_df)):\n",
    "    question = row[\"question\"]\n",
    "    retrieved_docs = vdb_manager.retrieve(question)\n",
    "    response = reader.generate([question], [retrieved_docs])[0][0][\"generated_text\"]\n",
    "    responses.append(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.2922905644961558),\n",
       " 'rouge2': np.float64(0.08503566174558237),\n",
       " 'rougeL': np.float64(0.1633339509506666),\n",
       " 'rougeLsum': np.float64(0.18049857927222107)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rouge.compute(predictions=responses, references=val_mini_df[\"answer\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.052588917299906786,\n",
       " 'precisions': [0.3598300970873786,\n",
       "  0.09737484737484738,\n",
       "  0.04176904176904177,\n",
       "  0.0203955500618047],\n",
       " 'brevity_penalty': 0.711476691707965,\n",
       " 'length_ratio': 0.7460389316432775,\n",
       " 'translation_length': 3296,\n",
       " 'reference_length': 4418}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bleu.compute(predictions=responses, references=val_mini_df[\"answer\"].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the scores above, our solution got a higher BLEU score compared to before (0.0525 > 0.0455) but a lower ROUGE score overall. This may imply that our solution is now getting more similar text blocks that appear on the reference, but that it no longer displays as much content as expected. This may be due to the fact that we're encouraging the model to only respond based on the context provided, which may not contain all the details necessary to construct a full response.\n",
    "\n",
    "One approach to mitigate this could be to increase the number of documents returned by our RAG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
