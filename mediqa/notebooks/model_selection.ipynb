{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "For modern NLP tasks, training models from scratch is often expensive, complex and not cost-effective. Thankfully, platforms like Huggingface provide free access to several models specialized in different tasks and topics. Because of this, we want to first select a few potential models that may provide good performance out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "val_df = pd.read_csv(\"../data/val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\n",
    "    \"ThisIs-Developer/Llama-2-GGML-Medical-Chatbot\",\n",
    "    # \"xdatasi/xdata-finetune-deepseek-reason-test-medical\"\n",
    "    \"WangCa/Qwen2.5-7B-Medicine\",\n",
    "    \"google/gemma-7b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The candidates specified above are finetuned models specifically designed for question-answering tasks in the medical domain. These models have different architectures, and according to their description they were trained on a variety of medical sources, with some having used up to 80 million documents for training. Additionally, a general-use case modern llm (`gemma-7b`) has been added as a baseline, to be able to compare performance between specialized models and general models.\n",
    "\n",
    "## Initial validation\n",
    "We want to see how the models perform as is. That is, if we only used the models without any additional changes, how close would they be to responding according to our validation dataset? For this, we're going to evaluate generated responses against the provided ones using BLEU and ROUGE metrics. These metrics are widely used for QA tasks, and in general work by counting the number of matching n-grams between the reference and generated responses. While they have their limitations (like not being well suited for long answers, or in case of BLEU not considering word order), they suffice for evaluating relative improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import pipeline\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "\n",
    "def evaluate_model(model_name, val_df):\n",
    "    pipe = pipeline(\"question-answering\", model=model_name)\n",
    "    predictions = val_df[\"question\"].apply(lambda x: pipe(x)[\"answer\"])\n",
    "    print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general obi wan kenobi\"]\n",
    "results = rouge.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rouge1': np.float64(0.8333333333333333),\n",
       " 'rouge2': np.float64(0.5),\n",
       " 'rougeL': np.float64(0.8333333333333333),\n",
       " 'rougeLsum': np.float64(0.8333333333333333)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "predictions = [\"hello there\", \"general kenobi\"]\n",
    "references = [\"hello there\", \"general obi wan kenobi\"]\n",
    "results = bleu.compute(predictions=predictions, references=references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bleu': 0.0,\n",
       " 'precisions': [1.0, 0.5, 0.0, 0.0],\n",
       " 'brevity_penalty': 0.6065306597126334,\n",
       " 'length_ratio': 0.6666666666666666,\n",
       " 'translation_length': 4,\n",
       " 'reference_length': 6}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
