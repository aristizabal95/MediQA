{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection\n",
    "\n",
    "For modern NLP tasks, training models from scratch is often expensive, complex and not cost-effective. Thankfully, platforms like Huggingface provide free access to several models specialized in different tasks and topics. Because of this, we want to first select a few potential models that may provide good performance out-of-the-box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from mediqa.config.core import DATASET_DIR\n",
    "\n",
    "val_df = pd.read_csv(Path(DATASET_DIR) / \"val.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidates = [\n",
    "    \"AdaptLLM/medicine-LLM\",\n",
    "    \"ritvik77/Medical_Doctor_AI_LoRA-Mistral-7B-Instruct_FullModel\",\n",
    "    \"ContactDoctor/Bio-Medical-Llama-3-8B\",\n",
    "    \"HuggingFaceH4/zephyr-7b-beta\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The candidates specified above are finetuned models specifically designed for question-answering tasks in the medical domain. These models have different architectures, and according to their description they were trained on a variety of medical sources, with some having used up to 80 million documents for training. Additionally, a general-use case modern llm (`gemma-7b`) has been added as a baseline, to be able to compare performance between specialized models and general models.\n",
    "\n",
    "## Initial validation\n",
    "We want to see how the models perform as is. That is, if we only used the models without any additional changes, how close would they be to responding according to our validation dataset? For this, we're going to evaluate generated responses against the provided ones using BLEU and ROUGE metrics. These metrics are widely used for QA tasks, and in general work by counting the number of matching n-grams between the reference and generated responses. While they have their limitations (like not being well suited for long answers, or in case of BLEU not considering word order), they suffice for evaluating relative improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aristizabal95/programming/MediQA/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "\n",
    "\n",
    "def load_pipeline(model_name):\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    )\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=bnb_config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    pipe = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        do_sample=True,\n",
    "        temperature=0.2,\n",
    "        repetition_penalty=1.1,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=1000,\n",
    "    )\n",
    "    return pipe\n",
    "\n",
    "def evaluate_pipe(pipe, df: pd.DataFrame, evaluators: list):\n",
    "    tqdm.pandas(desc=f\"Generating answers from {pipe.tokenizer.name_or_path}\")\n",
    "    predictions = df['question'].progress_apply(lambda x: pipe(x)[0]['generated_text'])\n",
    "    references = df['answer']\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for evaluator in evaluators:\n",
    "        result = evaluator.compute(predictions=predictions.tolist(), references=references.tolist())\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mini_df = val_df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import pipeline\n",
    "\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "bleu = evaluate.load(\"bleu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No pre-existing pipeline\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "try:\n",
    "    del pipe\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "except:\n",
    "    print(\"No pre-existing pipeline\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:08<00:00,  3.83it/s]\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n",
      "Device set to use cuda:0\n",
      "Generating answers from AdaptLLM/medicine-LLM:  55%|█████▌    | 11/20 [07:10<07:16, 48.45s/it]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "Generating answers from AdaptLLM/medicine-LLM: 100%|██████████| 20/20 [15:38<00:00, 46.93s/it]\n",
      "/home/aristizabal95/programming/MediQA/.venv/lib/python3.10/site-packages/transformers/quantizers/auto.py:212: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
      "  warnings.warn(warning_msg)\n",
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Device set to use cuda:0\n",
      "Generating answers from ritvik77/Medical_Doctor_AI_LoRA-Mistral-7B-Instruct_FullModel:  50%|█████     | 10/20 [02:52<03:31, 21.16s/it]"
     ]
    }
   ],
   "source": [
    "def get_benchmark_results(candidates) -> pd.DataFrame:\n",
    "    benchmark = {}\n",
    "\n",
    "    for candidate in candidates:\n",
    "        pipe = load_pipeline(candidate)\n",
    "        results = evaluate_pipe(pipe, val_mini_df, [rouge, bleu])\n",
    "        total_results = {}\n",
    "        for result_dict in results:\n",
    "            total_results.update(result_dict)\n",
    "        benchmark[candidate] = total_results\n",
    "\n",
    "    benchmark_df = pd.DataFrame(benchmark).T\n",
    "    return benchmark_df\n",
    "\n",
    "benchmark_df = get_benchmark_results([candidates[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mini_df.to_csv(\"../data/val_mini.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
